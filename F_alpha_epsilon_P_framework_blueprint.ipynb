{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4646f119",
   "metadata": {},
   "source": [
    "\n",
    "# **Fâ€“Î±â€“Îµâ€“P Framework Blueprint (Qualitativ hohes methodischâ€‘empirisches Werkzeug)**\n",
    "**A reusable Colab notebook** to evaluate models, experiments, or processes under heavy tails, sequential updates, and uncertaintyâ€”complete with explanations, code, calculations, and visualizations.\n",
    "\n",
    "> **What you get**\n",
    "> - **F (Concretion)**: measurable outcomes & empirical distribution \\(F\\).\n",
    "> - **Î± (Ingression)**: right/left tail indices via Hill-type estimators (heavy-tail sensitivity).\n",
    "> - **Îµ (Involution)**: quantile-based truncation & sequential update coherence.\n",
    "> - **P (Projection/Decision)**: scoring aggregator using quadratic / harmonic / geometric **quasinorm lenses**.\n",
    "> - **Visualizations**: distribution, QQ plots, lens scores, streaming updates, and the 3D evaluation cube.\n",
    ">\n",
    "> **How to use**\n",
    "> 1. Run the cells top-to-bottom.\n",
    "> 2. Either **simulate data** (default) or **load your own CSV** (instructions below).\n",
    "> 3. Adjust the tolerance \\( \\varepsilon \\) and the lens weights \\((\\alpha_2,\\alpha_{-1},\\alpha_0)\\).\n",
    "> 4. Read the inline commentary to transfer this to any formalized evaluation context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Imports ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, sqrt, exp\n",
    "from pathlib import Path\n",
    "\n",
    "# (No seaborn; one plot per figure; no explicit color choices.)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.width\", 120)\n",
    "pd.set_option(\"display.max_columns\", 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0fe9c",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data Ingestion (F â€“ Concretion)\n",
    "You can either **simulate** a dataset with heavy tails or **load your own**:\n",
    "\n",
    "- **Simulated**: mixture of light- and heavy-tailed components with optional multiplicative drift.\n",
    "- **Own data**: provide a CSV path and column name for the measurement/metric/residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d13cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === OPTION A: Simulated dataset ===\n",
    "n = 5000\n",
    "# Light-tailed component (normal)\n",
    "x_light = np.random.normal(loc=0.0, scale=1.0, size=n)\n",
    "# Heavy-tailed component (Pareto-like, shifted)\n",
    "# Pareto(alpha) with minimum x_m=1 => draw U, X = x_m * (1-U)^(-1/alpha)\n",
    "alpha_true = 3.0\n",
    "U = np.random.rand(n)\n",
    "x_heavy = (1 - U) ** (-1.0 / alpha_true)  # Pareto tail\n",
    "x_heavy = x_heavy - np.mean(x_heavy)      # center-ish\n",
    "\n",
    "# Mix & add multiplicative drift\n",
    "mix = np.where(np.random.rand(n) < 0.8, x_light, x_heavy)\n",
    "# Construct a \"residual-like\" series with drift in magnitude\n",
    "t = np.arange(n)\n",
    "drift = np.exp(0.001 * (t - n/2))  # mild multiplicative drift\n",
    "r_sim = mix * drift\n",
    "\n",
    "df = pd.DataFrame({\"r\": r_sim})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38a729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === OPTION B: Load your own CSV ===\n",
    "# Uncomment and set the path & column if you want to use your data.\n",
    "# csv_path = \"/content/your_data.csv\"\n",
    "# value_column = \"your_metric_column\"\n",
    "# df = pd.read_csv(csv_path)\n",
    "# df = df[[value_column]].rename(columns={value_column: \"r\"})\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0553bbde",
   "metadata": {},
   "source": [
    "## 2) Empirical Distribution $F$ and Îµ-Quantile Truncation (Îµ â€“ Involution)\n",
    "We compute the empirical CDF $F$ through quantiles and select **upper** and **lower** truncation thresholds:\n",
    "- $u_\\varepsilon = F^{-1}(1-\\varepsilon)$\n",
    "- $\\ell_\\varepsilon = F^{-1}(\\varepsilon)$\n",
    "\n",
    "This splits the sample into **bulk** and **tails** to guarantee $\\varepsilon$-controlled error on expectation-like functionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose epsilon (tolerance)\n",
    "epsilon = 0.01  # 1% tails on each side by default\n",
    "\n",
    "abs_r = np.abs(df[\"r\"].values)\n",
    "u_epsilon = np.quantile(abs_r, 1 - epsilon)  # upper cutoff\n",
    "l_epsilon = np.quantile(abs_r, epsilon)      # lower cutoff (near zero)\n",
    "\n",
    "bulk_mask = (abs_r >= l_epsilon) & (abs_r <= u_epsilon)\n",
    "tail_hi_mask = abs_r > u_epsilon\n",
    "tail_lo_mask = abs_r < l_epsilon\n",
    "\n",
    "n_bulk = bulk_mask.sum()\n",
    "n_hi = tail_hi_mask.sum()\n",
    "n_lo = tail_lo_mask.sum()\n",
    "\n",
    "print(f\"epsilon = {epsilon}\")\n",
    "print(f\"Upper cutoff u_epsilon = {u_epsilon:.4f}, Lower cutoff l_epsilon = {l_epsilon:.6f}\")\n",
    "print(f\"Counts -> bulk: {n_bulk}, upper tail: {n_hi}, lower 'near-zero' tail: {n_lo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bac525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize distribution and cutoffs\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "_ = plt.hist(abs_r, bins=60)\n",
    "_ = plt.axvline(u_epsilon, linestyle=\"--\")\n",
    "_ = plt.axvline(l_epsilon, linestyle=\"--\")\n",
    "_ = plt.title(\"Distribution of |r| with Îµ-cutoffs\")\n",
    "_ = plt.xlabel(\"|r|\")\n",
    "_ = plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fc99b",
   "metadata": {},
   "source": [
    "## 3) Tail Indices (Î±, Î²) â€“ Ingression\n",
    "Estimate heavy-tail behavior with Hill-type estimators:\n",
    "\n",
    "- **Right tail Î±** (large values): use top-k order statistics of $|r|$.\n",
    "- **Left tail Î²** (small values): transform $y_i = 1/|r_i|$ and apply Hill to $y$ (right tail of $y$ corresponds to left tail of $|r|$).\n",
    "\n",
    "> Note: Choose $k$ sensibly (e.g., 1â€“10% of data). We provide a simple heuristic and a basic stability plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b529df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hill_estimator(x_sorted_desc, k):\n",
    "    \"\"\"\n",
    "    Basic Hill estimator on descending-sorted positive array x.\n",
    "    Returns alpha_hat (tail index) for Pareto-like tail.\n",
    "    \"\"\"\n",
    "    x = x_sorted_desc\n",
    "    if k <= 1 or k >= len(x):\n",
    "        return np.nan\n",
    "    xk = x[:k]\n",
    "    xk_min = x[k]  # threshold (k+1-th largest)\n",
    "    logs = np.log(xk) - np.log(xk_min)\n",
    "    hill = 1.0 / (np.mean(logs) + 1e-12)\n",
    "    return hill\n",
    "\n",
    "# Prepare data for tails\n",
    "x_pos = abs_r[abs_r > 0]\n",
    "x_sorted = np.sort(x_pos)\n",
    "x_sorted_desc = x_sorted[::-1]\n",
    "\n",
    "# Heuristic k: top ~2% (at least 50 points)\n",
    "k_right = max(int(0.02 * len(x_sorted_desc)), 50)\n",
    "alpha_hat = hill_estimator(x_sorted_desc, k_right)\n",
    "\n",
    "# Left tail via y=1/|r| (large y => small |r|)\n",
    "y = 1.0 / x_pos\n",
    "y_sorted = np.sort(y)\n",
    "y_sorted_desc = y_sorted[::-1]\n",
    "k_left = max(int(0.02 * len(y_sorted_desc)), 50)\n",
    "beta_hat = hill_estimator(y_sorted_desc, k_left)\n",
    "\n",
    "print(f\"Hill tail index estimates: alpha (right tail) â‰ˆ {alpha_hat:.3f}, beta (left/near-zero) â‰ˆ {beta_hat:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stability scan for k\n",
    "k_vals = np.unique(np.linspace(30, max(60, int(0.1*len(x_sorted_desc))), 12, dtype=int))\n",
    "alpha_series = [hill_estimator(x_sorted_desc, k) for k in k_vals]\n",
    "beta_series  = [hill_estimator(y_sorted_desc, k) for k in k_vals]\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "_ = plt.plot(k_vals, alpha_series, marker=\"o\")\n",
    "_ = plt.axhline(alpha_hat, linestyle=\"--\")\n",
    "_ = plt.title(\"Right-tail Hill Î± vs k\")\n",
    "_ = plt.xlabel(\"k (top order stats)\")\n",
    "_ = plt.ylabel(\"Î± estimate\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "_ = plt.plot(k_vals, beta_series, marker=\"o\")\n",
    "_ = plt.axhline(beta_hat, linestyle=\"--\")\n",
    "_ = plt.title(\"Left-tail Hill Î² vs k (via y=1/|r|)\")\n",
    "_ = plt.xlabel(\"k (top order stats of y)\")\n",
    "_ = plt.ylabel(\"Î² estimate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee598f",
   "metadata": {},
   "source": [
    "## 4) Quasinorm Lenses (Quadratic $L^2$, Harmonic $L^{-1}$, Geometric $L^0$)\n",
    "Compute on **bulk** (Îµ-truncated) data to guarantee finite, stable contributions.\n",
    "\n",
    "- $\\|r\\|_{(2)} = \\sqrt{\\frac{1}{n}\\sum r_i^2}$\n",
    "- $\\|r\\|_{(-1)} = \\left(\\frac{1}{n}\\sum \\frac{1}{|r_i|}\\right)^{-1}$\n",
    "- $\\|r\\|_{(0)} = \\exp\\left(\\frac{1}{n}\\sum \\log|r_i|\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebc646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lens_L2(x):\n",
    "    return np.sqrt(np.mean(x**2))\n",
    "\n",
    "def lens_Lminus1(x):\n",
    "    # avoid division by zero in bulk by design; add tiny epsilon safeguard\n",
    "    eps = 1e-12\n",
    "    return 1.0 / np.mean(1.0 / (np.abs(x) + eps))\n",
    "\n",
    "def lens_L0(x):\n",
    "    eps = 1e-12\n",
    "    return np.exp(np.mean(np.log(np.abs(x) + eps)))\n",
    "\n",
    "r_bulk = df[\"r\"].values[bulk_mask]\n",
    "L2 = lens_L2(r_bulk)\n",
    "Lminus1 = lens_Lminus1(r_bulk)\n",
    "L0 = lens_L0(r_bulk)\n",
    "\n",
    "print(f\"L2 (quadratic)     = {L2:.4f}\")\n",
    "print(f\"L^{-1} (harmonic)  = {Lminus1:.4f}\")\n",
    "print(f\"L^0 (geometric)    = {L0:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize lens values\n",
    "vals = [L2, Lminus1, L0]\n",
    "labels = [\"L2 (quadratic)\", \"L^{-1} (harmonic)\", \"L^0 (geometric)\"]\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "_ = plt.bar(labels, vals)\n",
    "_ = plt.title(\"Quasinorm lenses on Îµ-bulk\")\n",
    "_ = plt.ylabel(\"value\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495dd69",
   "metadata": {},
   "source": [
    "## 5) Projection / Aggregation (P)\n",
    "Aggregate the lenses with weights $(\\alpha_2,\\alpha_{-1},\\alpha_0)$ to get a **single score** $\\mathcal{E}$.\n",
    "\n",
    "Choose weights according to context:\n",
    "- Risk-averse (emphasize extremes): higher weight on $L^2$.\n",
    "- Bottleneck-averse (avoid small values): higher weight on $L^{-1}$.\n",
    "- Multiplicative growth consistency: higher weight on $L^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha2, alpha_minus1, alpha0 = 1/3, 1/3, 1/3  # balanced by default\n",
    "w_sum = alpha2 + alpha_minus1 + alpha0\n",
    "alpha2, alpha_minus1, alpha0 = alpha2/w_sum, alpha_minus1/w_sum, alpha0/w_sum\n",
    "\n",
    "E_score = alpha2*L2 + alpha_minus1*Lminus1 + alpha0*L0\n",
    "print(f\"Weights: alpha2={alpha2:.3f}, alpha-1={alpha_minus1:.3f}, alpha0={alpha0:.3f}\")\n",
    "print(f\"Aggregate score ð“” = {E_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43413ac",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Sequential Updates (Involution â€“ coherence over time)\n",
    "We simulate a **streaming** scenario: compute the three lenses and the aggregate score on a rolling window to show stable update behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ef94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window = 500\n",
    "r = df[\"r\"].values\n",
    "t_vals = []\n",
    "E_vals, L2_vals, Lm1_vals, L0_vals = [], [], [], []\n",
    "\n",
    "for end in range(window, len(r)+1, 50):\n",
    "    seg = r[end-window:end]\n",
    "    abs_seg = np.abs(seg)\n",
    "    u = np.quantile(abs_seg, 1 - epsilon)\n",
    "    l = np.quantile(abs_seg, epsilon)\n",
    "    bulk = seg[(abs_seg >= l) & (abs_seg <= u)]\n",
    "    if len(bulk) < 10:\n",
    "        continue\n",
    "    l2 = lens_L2(bulk)\n",
    "    lm1 = lens_Lminus1(bulk)\n",
    "    l0 = lens_L0(bulk)\n",
    "    score = alpha2*l2 + alpha_minus1*lm1 + alpha0*l0\n",
    "    t_vals.append(end)\n",
    "    L2_vals.append(l2); Lm1_vals.append(lm1); L0_vals.append(l0); E_vals.append(score)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "_ = plt.plot(t_vals, E_vals, marker=\"o\")\n",
    "_ = plt.title(\"Aggregate score ð“” over time (rolling window)\")\n",
    "_ = plt.xlabel(\"time index\")\n",
    "_ = plt.ylabel(\"ð“”\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "_ = plt.plot(t_vals, L2_vals, label=\"L2\")\n",
    "_ = plt.plot(t_vals, Lm1_vals, label=\"L^{-1}\")\n",
    "_ = plt.plot(t_vals, L0_vals, label=\"L^0\")\n",
    "_ = plt.title(\"Lens values over time\")\n",
    "_ = plt.xlabel(\"time index\")\n",
    "_ = plt.ylabel(\"lens value\")\n",
    "_ = plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7dd8f",
   "metadata": {},
   "source": [
    "\n",
    "## 7) 3D Evaluation Cube (C, I, V)\n",
    "A simple 3D illustration: the **necessary corner (1,1,1)** corresponds to fair evaluation (all three axes satisfied). We place our current evaluation there once the components are computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e435447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cube drawing similar to earlier, minimalistic\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "\n",
    "verts = np.array([[0,0,0],[1,0,0],[0,1,0],[1,1,0],[0,0,1],[1,0,1],[0,1,1],[1,1,1]])\n",
    "edges = [(0,1),(0,2),(0,4),(1,3),(1,5),(2,3),(2,6),(3,7),(4,5),(4,6),(5,7),(6,7)]\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for e in edges:\n",
    "    pts = verts[list(e)]\n",
    "    ax.plot(pts[:,0], pts[:,1], pts[:,2])\n",
    "for v in verts:\n",
    "    ax.scatter(v[0], v[1], v[2], s=30)\n",
    "ax.text(1.03,1.03,1.03,\"Fair (1,1,1)\")\n",
    "ax.set_xlabel(\"C (Concretion)\")\n",
    "ax.set_ylabel(\"I (Ingression)\")\n",
    "ax.set_zlabel(\"V (Involution)\")\n",
    "ax.set_xticks([0,1]); ax.set_yticks([0,1]); ax.set_zticks([0,1])\n",
    "ax.view_init(elev=20, azim=30)\n",
    "plt.title(\"Evaluation Cube\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516841a",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Summary Report\n",
    "This cell prints a concise, copy-paste ready summary for documentation or decision memos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfed6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_report():\n",
    "    lines = []\n",
    "    lines.append(\"=== Fâ€“Î±â€“Îµâ€“P Evaluation Summary ===\")\n",
    "    lines.append(f\"epsilon (tolerance): {epsilon}\")\n",
    "    lines.append(f\"Right tail index Î± (Hill): {alpha_hat:.3f}\")\n",
    "    lines.append(f\"Left tail index Î²  (Hill via y=1/|r|): {beta_hat:.3f}\")\n",
    "    lines.append(f\"Upper cutoff u_epsilon: {u_epsilon:.6f}\")\n",
    "    lines.append(f\"Lower cutoff l_epsilon: {l_epsilon:.6f}\")\n",
    "    lines.append(f\"Bulk size: {n_bulk}, Upper tail size: {n_hi}, Lower tail size: {n_lo}\")\n",
    "    lines.append(\"--- Lenses on Îµ-bulk ---\")\n",
    "    lines.append(f\"L2 (quadratic): {L2:.6f}\")\n",
    "    lines.append(f\"L^-1 (harmonic): {Lminus1:.6f}\")\n",
    "    lines.append(f\"L^0 (geometric): {L0:.6f}\")\n",
    "    lines.append(\"--- Weights ---\")\n",
    "    lines.append(f\"alpha2={alpha2:.3f}, alpha-1={alpha_minus1:.3f}, alpha0={alpha0:.3f}\")\n",
    "    lines.append(f\"Aggregate score ð“”: {E_score:.6f}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "print(summary_report())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a5e0d",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Reusable API (apply to any metric/residual column)\n",
    "Use `evaluate_series(series, epsilon, weights)` on your own data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_series(series: np.ndarray, epsilon: float = 0.01, weights=(1/3,1/3,1/3)):\n",
    "    series = np.asarray(series).astype(float)\n",
    "    abs_s = np.abs(series[series!=0])\n",
    "    if len(abs_s) < 100:\n",
    "        raise ValueError(\"Need at least 100 non-zero points for stable tail/lens estimates.\")\n",
    "    u = np.quantile(abs_s, 1 - epsilon)\n",
    "    l = np.quantile(abs_s, epsilon)\n",
    "    bulk = series[(np.abs(series) >= l) & (np.abs(series) <= u)]\n",
    "    # Hill estimates\n",
    "    x_sorted_desc = np.sort(abs_s)[::-1]\n",
    "    y_sorted_desc = np.sort(1.0/abs_s)[::-1]\n",
    "    k_right = max(int(0.02 * len(x_sorted_desc)), 50)\n",
    "    k_left  = max(int(0.02 * len(y_sorted_desc)), 50)\n",
    "    a_hat = hill_estimator(x_sorted_desc, k_right)\n",
    "    b_hat = hill_estimator(y_sorted_desc, k_left)\n",
    "    # lenses\n",
    "    L2v = lens_L2(bulk)\n",
    "    Lm1v = lens_Lminus1(bulk)\n",
    "    L0v = lens_L0(bulk)\n",
    "    w = np.array(weights, dtype=float)\n",
    "    w = w / np.sum(w)\n",
    "    E = w[0]*L2v + w[1]*Lm1v + w[2]*L0v\n",
    "    return {\n",
    "        \"epsilon\": epsilon,\n",
    "        \"u_epsilon\": float(u),\n",
    "        \"l_epsilon\": float(l),\n",
    "        \"alpha_hat\": float(a_hat),\n",
    "        \"beta_hat\": float(b_hat),\n",
    "        \"L2\": float(L2v),\n",
    "        \"L-1\": float(Lm1v),\n",
    "        \"L0\": float(L0v),\n",
    "        \"weights\": w.tolist(),\n",
    "        \"E\": float(E),\n",
    "        \"n_bulk\": int(len(bulk))\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "res = evaluate_series(df[\"r\"].values, epsilon=0.01, weights=(0.5,0.25,0.25))\n",
    "res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
